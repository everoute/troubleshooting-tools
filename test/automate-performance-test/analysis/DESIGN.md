# æ€§èƒ½æµ‹è¯•æ•°æ®åˆ†æå·¥å…· - è®¾è®¡æ–‡æ¡£

## ğŸ“Œ å½“å‰ç‰ˆæœ¬ï¼šv1.0ï¼ˆ2025-10-22ï¼‰

### å®ç°çŠ¶æ€
- âœ… æ‰€æœ‰æ ¸å¿ƒæ¨¡å—å·²å®ç°
- âœ… Multi-stream æ•°æ®ç´¯åŠ å·²ä¿®å¤
- âœ… PPS å®¹é”™å¤„ç†å·²å®ç°
- âœ… åŒæŠ¥å‘Šç³»ç»Ÿå·²éƒ¨ç½²
- â³ Server ç«¯æ•°æ®å®šä½å¾…ä¼˜åŒ–
- â³ èµ„æºç›‘æ§æ—¶é—´åŒ¹é…å¾…ä¼˜åŒ–

### å…³é”®ç‰¹æ€§
1. **æ™ºèƒ½æ•°æ®å®šä½**ï¼šè‡ªåŠ¨è¯†åˆ« Host/VM æµ‹è¯•ç±»å‹
2. **å®¹é”™è§£æ**ï¼šMulti-stream timing è‡ªé€‚åº”ï¼ŒPPS packet_size å®¹é”™
3. **åŒæŠ¥å‘Šç³»ç»Ÿ**ï¼šåˆå¹¶æŠ¥å‘Šï¼ˆ37åˆ—ï¼‰+ åˆ†ç¦»æŠ¥å‘Šï¼ˆ5ä¸ªä¸“é¡¹æ–‡ä»¶ï¼‰
4. **çµæ´»é…ç½®**ï¼šæ”¯æŒ YAML é…ç½®å’Œå‘½ä»¤è¡Œå‚æ•°

---

## 1. æ¶æ„æ¦‚è§ˆ

### 1.1 ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Main Application                        â”‚
â”‚                  (analyze_performance.py)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”œâ”€â”€â”€ Configuration Manager (config.yaml)
               â”‚
               â”œâ”€â”€â”€ Data Locator
               â”‚    â”œâ”€â”€â”€ å®šä½æ‰€æœ‰æ•°æ®æ–‡ä»¶è·¯å¾„
               â”‚    â””â”€â”€â”€ è‡ªåŠ¨è¯†åˆ« Host/VM æµ‹è¯•ç±»å‹
               â”‚
               â”œâ”€â”€â”€ Performance Data Parser
               â”‚    â”œâ”€â”€â”€ Latency Parser
               â”‚    â”œâ”€â”€â”€ Throughput Parser (æ”¯æŒ Multi-stream)
               â”‚    â””â”€â”€â”€ PPS Parser (æ”¯æŒå®¹é”™)
               â”‚
               â”œâ”€â”€â”€ Resource Monitor Parser
               â”‚    â”œâ”€â”€â”€ Pidstat Log Parser
               â”‚    â””â”€â”€â”€ Time Range Filter
               â”‚
               â”œâ”€â”€â”€ Log Size Parser
               â”‚
               â”œâ”€â”€â”€ Baseline Comparator
               â”‚    â””â”€â”€â”€ Diff Calculator
               â”‚
               â”œâ”€â”€â”€ Report Generator (åˆå¹¶æŠ¥å‘Š)
               â”‚    â”œâ”€â”€â”€ CSV Generator (37åˆ—)
               â”‚    â””â”€â”€â”€ Markdown Generator
               â”‚
               â””â”€â”€â”€ Report Generator V2 (åˆ†ç¦»æŠ¥å‘Šï¼Œæ¨è)
                    â”œâ”€â”€â”€ Latency Report (13åˆ—)
                    â”œâ”€â”€â”€ Throughput Report (15åˆ—)
                    â”œâ”€â”€â”€ PPS Report (15åˆ—)
                    â”œâ”€â”€â”€ Resources Report (20åˆ—)
                    â””â”€â”€â”€ Overview Markdown
```

### 1.2 æ¨¡å—ä¾èµ–å…³ç³»

```
analyze_performance.py (ä¸»ç¨‹åº)
    â†“
config.yaml (é…ç½®)
    â†“
data_locator.py (æ•°æ®å®šä½)
    â†“
parsers/
    â”œâ”€â”€ performance_parser.py (æ€§èƒ½æ•°æ®è§£æ)
    â”œâ”€â”€ resource_parser.py (èµ„æºç›‘æ§è§£æ)
    â””â”€â”€ logsize_parser.py (æ—¥å¿—å¤§å°è§£æ)
    â†“
comparator.py (åŸºçº¿å¯¹æ¯”)
    â†“
report_generator.py (æŠ¥å‘Šç”Ÿæˆ)
```

### 1.3 ç›®å½•ç»“æ„

```
analysis/
â”œâ”€â”€ REQUIREMENTS.md           # éœ€æ±‚æ–‡æ¡£
â”œâ”€â”€ DESIGN.md                # è®¾è®¡æ–‡æ¡£ï¼ˆæœ¬æ–‡æ¡£ï¼‰
â”œâ”€â”€ README.md                # ä½¿ç”¨è¯´æ˜
â”œâ”€â”€ config.yaml              # é…ç½®æ–‡ä»¶
â”œâ”€â”€ analyze_performance.py   # ä¸»ç¨‹åº
â”œâ”€â”€ src/                     # æºä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_locator.py      # æ•°æ®å®šä½å™¨
â”‚   â”œâ”€â”€ comparator.py        # åŸºçº¿å¯¹æ¯”å™¨
â”‚   â”œâ”€â”€ report_generator.py  # æŠ¥å‘Šç”Ÿæˆå™¨
â”‚   â”œâ”€â”€ utils.py             # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ parsers/             # è§£æå™¨æ¨¡å—
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ performance_parser.py  # æ€§èƒ½æ•°æ®è§£æå™¨
â”‚       â”œâ”€â”€ resource_parser.py     # èµ„æºç›‘æ§è§£æå™¨
â”‚       â””â”€â”€ logsize_parser.py      # æ—¥å¿—å¤§å°è§£æå™¨
â”œâ”€â”€ tests/                   # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_data_locator.py
â”‚   â”œâ”€â”€ test_parsers.py
â”‚   â””â”€â”€ test_comparator.py
â””â”€â”€ output/                  # è¾“å‡ºç›®å½•
    â””â”€â”€ .gitkeep
```

---

## 2. æ ¸å¿ƒæ¨¡å—è®¾è®¡

### 2.1 Data Locatorï¼ˆæ•°æ®å®šä½å™¨ï¼‰

**èŒè´£ï¼š** æ ¹æ® iteration å’Œ tool case åç§°ï¼Œå®šä½æ‰€æœ‰ç›¸å…³æ•°æ®æ–‡ä»¶è·¯å¾„

**è¾“å…¥ï¼š**
- `iteration_path`: iteration ç›®å½•è·¯å¾„
- `tool_case_name`: tool case åç§°

**è¾“å‡ºï¼š**
```python
{
    "test_type": "host" | "vm",
    "client": {
        "latency": {
            "tcp_rr": "/path/to/latency/tcp_rr_*/latency_tcp_rr.txt",
            "udp_rr": "/path/to/latency/udp_rr_*/latency_udp_rr.txt"
        },
        "throughput": {
            "single": {
                "json": "/path/to/throughput/single_*/throughput_single_tcp.json",
                "timing": "/path/to/throughput/single_*/throughput_single_timing.log"
            },
            "multi": {
                "json_files": ["/path/to/throughput/multi_*/throughput_multi_tcp_port_*.json"],
                "timing": "/path/to/throughput/multi_*/throughput_multi_timing.log"
            }
        },
        "pps": {
            "single": {...},
            "multi": {...}
        }
    },
    "server": {
        "latency": {...},
        "throughput": {...},
        "pps": {...},
        "ebpf_monitoring": {
            "resource_monitor": "/path/to/ebpf_monitoring/ebpf_resource_monitor_*.log",
            "logsize_monitor": "/path/to/ebpf_monitoring/ebpf_logsize_monitor_*.log"
        }
    }
}
```

**å…³é”®é€»è¾‘ï¼š**
1. è‡ªåŠ¨è¯†åˆ«æµ‹è¯•ç±»å‹ï¼ˆhost/vmï¼‰
   - æ£€æŸ¥ `host-server/performance-test-results/ebpf/{tool_case_name}/host/` æ˜¯å¦å­˜åœ¨
   - æ£€æŸ¥ `vm-server/performance-test-results/ebpf/{tool_case_name}/vm/` æ˜¯å¦å­˜åœ¨
2. æ ¹æ®æµ‹è¯•ç±»å‹æ„å»ºä¸åŒçš„è·¯å¾„
3. ä½¿ç”¨ glob æ¨¡å¼åŒ¹é…æ—¶é—´æˆ³æ–‡ä»¶å

**é”™è¯¯å¤„ç†ï¼š**
- å¦‚æœå…³é”®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å› None å¹¶è®°å½•è­¦å‘Š
- å¦‚æœæ‰¾åˆ°å¤šä¸ªåŒ¹é…æ–‡ä»¶ï¼Œé€‰æ‹©æœ€æ–°çš„ï¼ˆæŒ‰æ–‡ä»¶åæ’åºï¼‰

**ç±»è®¾è®¡ï¼š**
```python
class DataLocator:
    def __init__(self, iteration_path: str):
        self.iteration_path = iteration_path

    def locate_tool_case(self, tool_case_name: str) -> dict:
        """å®šä½å•ä¸ª tool case çš„æ‰€æœ‰æ•°æ®æ–‡ä»¶"""
        pass

    def locate_baseline(self, test_type: str) -> dict:
        """å®šä½ baseline æ•°æ®æ–‡ä»¶"""
        pass

    def _detect_test_type(self, tool_case_name: str) -> str:
        """æ£€æµ‹æµ‹è¯•ç±»å‹ï¼ˆhost/vmï¼‰"""
        pass

    def _find_latest_file(self, pattern: str) -> str:
        """æ‰¾åˆ°æœ€æ–°çš„åŒ¹é…æ–‡ä»¶"""
        pass
```

---

### 2.2 Performance Data Parserï¼ˆæ€§èƒ½æ•°æ®è§£æå™¨ï¼‰

**èŒè´£ï¼š** è§£æå»¶è¿Ÿã€ååé‡ã€PPS æµ‹è¯•ç»“æœ

#### 2.2.1 Latency Parser

**è¾“å…¥ï¼š** latency_tcp_rr.txt æ–‡ä»¶è·¯å¾„

**è¾“å‡ºï¼š**
```python
{
    "min_us": 55,
    "mean_us": 112.59,
    "max_us": 19236
}
```

**è§£æé€»è¾‘ï¼š**
```python
def parse_latency(file_path: str) -> dict:
    with open(file_path, 'r') as f:
        lines = f.readlines()

    # è·³è¿‡å‰ä¸¤è¡Œï¼ˆheaderï¼‰
    data_line = lines[2].strip()
    values = data_line.split(',')

    return {
        "min_us": float(values[0]),
        "mean_us": float(values[1]),
        "max_us": float(values[2])
    }
```

#### 2.2.2 Throughput Parser

**è¾“å…¥ï¼š**
- JSON æ–‡ä»¶è·¯å¾„ï¼ˆå•ä¸ªæˆ–å¤šä¸ªï¼‰
- timing.log æ–‡ä»¶è·¯å¾„

**è¾“å‡ºï¼š**
```python
{
    "throughput_gbps": 12.05,
    "start_time": "2025-10-21 14:12:43.774",
    "end_time": "2025-10-21 14:12:53.890",
    "start_epoch": 1761055963,
    "end_epoch": 1761055973
}
```

**è§£æé€»è¾‘ï¼š**
```python
def parse_throughput(json_path: str, timing_path: str) -> dict:
    # 1. è§£æ iperf3 JSON
    with open(json_path, 'r') as f:
        data = json.load(f)

    bps = data["end"]["sum_sent"]["bits_per_second"]
    throughput_gbps = bps / 1e9

    # 2. è§£æ timing log
    with open(timing_path, 'r') as f:
        lines = f.readlines()

    start_time = parse_line(lines[1], "Start: ")
    end_time = parse_line(lines[2], "End: ")

    return {
        "throughput_gbps": round(throughput_gbps, 2),
        "start_time": start_time,
        "end_time": end_time,
        "start_epoch": datetime_to_epoch(start_time),
        "end_epoch": datetime_to_epoch(end_time)
    }
```

**Multi-stream å¤„ç†ï¼š**
```python
def parse_throughput_multi(json_paths: list, timing_path: str) -> dict:
    # ç´¯åŠ æ‰€æœ‰ stream çš„ååé‡
    total_bps = 0
    for json_path in json_paths:
        with open(json_path, 'r') as f:
            data = json.load(f)
        total_bps += data["end"]["sum_sent"]["bits_per_second"]

    throughput_gbps = total_bps / 1e9

    # æ³¨æ„ï¼šMulti-stream çš„ timing æ ¼å¼ä¸ PPS ç›¸åŒï¼ˆProcess_Start/Actual_Launch/Test_Endï¼‰
    # è€Œä¸æ˜¯ Start/End æ ¼å¼ï¼
    # éœ€è¦ä½¿ç”¨ parse_timing_log(timing_path, "pps") è€Œä¸æ˜¯ "throughput"
    timing = parse_timing_log(timing_path, "pps")  # ä¿®æ­£ï¼šä½¿ç”¨ PPS æ ¼å¼
    # ...
```

#### 2.2.3 PPS Parser

**è¾“å…¥ï¼š** åŒ Throughput Parser

**è¾“å‡ºï¼š**
```python
{
    "pps": 4500000,
    "throughput_gbps": 2.304,  # è¾…åŠ©ä¿¡æ¯
    "packet_size_bytes": 64,
    "start_time": "2025-10-21 14:13:41.966",
    "end_time": "2025-10-21 14:13:54.085",
    "start_epoch": 1761056021,
    "end_epoch": 1761056034
}
```

**è§£æé€»è¾‘ï¼š**
```python
def parse_pps(json_path: str, timing_path: str) -> dict:
    # 1. è§£æ iperf3 JSON
    with open(json_path, 'r') as f:
        data = json.load(f)

    bps = data["end"]["sum_sent"]["bits_per_second"]

    # è·å– packet_sizeï¼ˆå®¹é”™å¤„ç†ï¼‰
    packet_size = None
    # å°è¯•ä» test_start è·å–
    if "test_start" in data and "blksize" in data["test_start"]:
        packet_size = data["test_start"]["blksize"]
    # å°è¯•ä» start.test_start è·å–
    elif "start" in data and "test_start" in data["start"] and "blksize" in data["start"]["test_start"]:
        packet_size = data["start"]["test_start"]["blksize"]
    # é»˜è®¤å€¼
    else:
        packet_size = 64  # é»˜è®¤ PPS æµ‹è¯•ä½¿ç”¨ 64 å­—èŠ‚åŒ…

    pps = bps / (packet_size * 8)
    throughput_gbps = bps / 1e9

    # 2. è§£æ timing logï¼ˆæ³¨æ„ï¼šPPS timing æ ¼å¼ä¸åŒï¼‰
    # Process_Start / Actual_Launch / Test_End
    with open(timing_path, 'r') as f:
        lines = f.readlines()

    start_time = parse_line(lines[2], "Actual_Launch: ")
    end_time = parse_line(lines[3], "Test_End: ")

    return {
        "pps": int(pps),
        "throughput_gbps": round(throughput_gbps, 2),
        "packet_size_bytes": packet_size,
        "start_time": start_time,
        "end_time": end_time,
        "start_epoch": datetime_to_epoch(start_time),
        "end_epoch": datetime_to_epoch(end_time)
    }
```

**ç±»è®¾è®¡ï¼š**
```python
class PerformanceParser:
    @staticmethod
    def parse_latency(file_path: str) -> dict:
        pass

    @staticmethod
    def parse_throughput_single(json_path: str, timing_path: str) -> dict:
        pass

    @staticmethod
    def parse_throughput_multi(json_paths: list, timing_path: str) -> dict:
        pass

    @staticmethod
    def parse_pps_single(json_path: str, timing_path: str) -> dict:
        pass

    @staticmethod
    def parse_pps_multi(json_paths: list, timing_path: str) -> dict:
        pass

    @staticmethod
    def parse_all(paths: dict) -> dict:
        """è§£ææ‰€æœ‰æ€§èƒ½æ•°æ®"""
        pass
```

---

### 2.3 Resource Monitor Parserï¼ˆèµ„æºç›‘æ§è§£æå™¨ï¼‰

**èŒè´£ï¼š** è§£æ pidstat èµ„æºç›‘æ§æ—¥å¿—ï¼Œæ”¯æŒæ—¶é—´èŒƒå›´è¿‡æ»¤

**è¾“å…¥ï¼š**
- `log_path`: resource monitor æ—¥å¿—è·¯å¾„
- `time_ranges`: æ—¶é—´èŒƒå›´åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

**è¾“å‡ºï¼š**
```python
{
    "full_cycle": {
        "max_rss_kb": 146992,
        "max_vsz_kb": 359164,
        "max_rss_timestamp": 1761055963,
        "max_vsz_timestamp": 1761055963
    },
    "time_range_stats": {
        "pps_single": {
            "cpu": {
                "avg_percent": 15.3,
                "max_percent": 20.0,
                "min_percent": 0.0
            },
            "memory": {
                "avg_rss_kb": 146992,
                "max_rss_kb": 146992
            },
            "page_faults": {
                "avg_minflt_per_sec": 25.5,
                "max_minflt_per_sec": 75.0
            },
            "sample_count": 5
        },
        "throughput_multi": {...}
    }
}
```

**è§£æé€»è¾‘ï¼š**
```python
def parse_resource_monitor(log_path: str, time_ranges: dict = None) -> dict:
    records = []

    with open(log_path, 'r') as f:
        for line in f:
            if line.startswith('#') or not line.strip():
                continue

            # è§£æ pidstat è¾“å‡ºæ ¼å¼
            parts = line.split()
            if len(parts) < 13:
                continue

            record = {
                "timestamp": int(parts[0]),
                "cpu_percent": float(parts[6]),
                "rss_kb": int(parts[11]),
                "vsz_kb": int(parts[10]),
                "minflt_per_sec": float(parts[8]),
                "mem_percent": float(parts[12])
            }
            records.append(record)

    # å…¨å‘¨æœŸç»Ÿè®¡
    full_cycle = calculate_full_cycle_stats(records)

    # æ—¶é—´èŒƒå›´ç»Ÿè®¡
    time_range_stats = {}
    if time_ranges:
        for name, (start_epoch, end_epoch) in time_ranges.items():
            filtered = [r for r in records
                       if start_epoch <= r["timestamp"] <= end_epoch]
            time_range_stats[name] = calculate_stats(filtered)

    return {
        "full_cycle": full_cycle,
        "time_range_stats": time_range_stats
    }

def calculate_stats(records: list) -> dict:
    if not records:
        return None

    cpu_values = [r["cpu_percent"] for r in records]
    rss_values = [r["rss_kb"] for r in records]
    minflt_values = [r["minflt_per_sec"] for r in records]

    return {
        "cpu": {
            "avg_percent": round(sum(cpu_values) / len(cpu_values), 2),
            "max_percent": round(max(cpu_values), 2),
            "min_percent": round(min(cpu_values), 2)
        },
        "memory": {
            "avg_rss_kb": int(sum(rss_values) / len(rss_values)),
            "max_rss_kb": max(rss_values)
        },
        "page_faults": {
            "avg_minflt_per_sec": round(sum(minflt_values) / len(minflt_values), 2),
            "max_minflt_per_sec": round(max(minflt_values), 2)
        },
        "sample_count": len(records)
    }
```

**ç±»è®¾è®¡ï¼š**
```python
class ResourceParser:
    @staticmethod
    def parse(log_path: str, time_ranges: dict = None) -> dict:
        pass

    @staticmethod
    def _parse_pidstat_line(line: str) -> dict:
        """è§£æå•è¡Œ pidstat è¾“å‡º"""
        pass

    @staticmethod
    def _calculate_stats(records: list) -> dict:
        """è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡"""
        pass
```

---

### 2.4 Log Size Parserï¼ˆæ—¥å¿—å¤§å°è§£æå™¨ï¼‰

**èŒè´£ï¼š** è§£æ eBPF æ—¥å¿—å¤§å°ç›‘æ§æ•°æ®

**è¾“å…¥ï¼š** logsize monitor æ—¥å¿—è·¯å¾„

**è¾“å‡ºï¼š**
```python
{
    "final_size_bytes": 0,
    "final_size_human": "0B",
    "growth_rate_bytes_per_sec": 0,
    "sample_count": 48
}
```

**è§£æé€»è¾‘ï¼š**
```python
def parse_logsize(log_path: str) -> dict:
    records = []

    with open(log_path, 'r') as f:
        for line in f:
            if line.startswith('#') or not line.strip():
                continue

            parts = line.split()
            if len(parts) < 3:
                continue

            timestamp_str = parts[0] + ' ' + parts[1]
            size_bytes = int(parts[2])

            records.append({
                "timestamp": parse_datetime(timestamp_str),
                "size_bytes": size_bytes
            })

    if not records:
        return None

    # æœ€ç»ˆå¤§å°
    final_size = records[-1]["size_bytes"]

    # è®¡ç®—å¢é•¿ç‡
    if len(records) > 1:
        duration = (records[-1]["timestamp"] - records[0]["timestamp"]).total_seconds()
        growth = final_size - records[0]["size_bytes"]
        growth_rate = growth / duration if duration > 0 else 0
    else:
        growth_rate = 0

    return {
        "final_size_bytes": final_size,
        "final_size_human": humanize_bytes(final_size),
        "growth_rate_bytes_per_sec": round(growth_rate, 2),
        "sample_count": len(records)
    }
```

**ç±»è®¾è®¡ï¼š**
```python
class LogSizeParser:
    @staticmethod
    def parse(log_path: str) -> dict:
        pass
```

---

### 2.5 Baseline Comparatorï¼ˆåŸºçº¿å¯¹æ¯”å™¨ï¼‰

**èŒè´£ï¼š** å¯¹æ¯” eBPF tool case å’Œ baseline çš„æ€§èƒ½å·®å¼‚

**è¾“å…¥ï¼š**
- `ebpf_data`: eBPF tool case çš„æ€§èƒ½æ•°æ®
- `baseline_data`: Baseline æ€§èƒ½æ•°æ®

**è¾“å‡ºï¼š**
```python
{
    "latency": {
        "tcp_rr_mean_us": {
            "ebpf": 112.59,
            "baseline": 105.00,
            "diff_absolute": 7.59,
            "diff_percent": 7.23
        }
    },
    "throughput": {
        "client_single_gbps": {
            "ebpf": 12.05,
            "baseline": 12.50,
            "diff_absolute": -0.45,
            "diff_percent": -3.60
        },
        "server_multi_gbps": {...}
    },
    "pps": {...}
}
```

**å¯¹æ¯”é€»è¾‘ï¼š**
```python
def compare(ebpf_data: dict, baseline_data: dict) -> dict:
    result = {}

    # å»¶è¿Ÿå¯¹æ¯”ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
    if "latency" in ebpf_data and "latency" in baseline_data:
        result["latency"] = {}
        for protocol in ["tcp_rr", "udp_rr"]:
            if protocol in ebpf_data["latency"]:
                ebpf_val = ebpf_data["latency"][protocol]["mean_us"]
                baseline_val = baseline_data["latency"][protocol]["mean_us"]
                result["latency"][f"{protocol}_mean_us"] = calculate_diff(
                    ebpf_val, baseline_val
                )

    # ååé‡å¯¹æ¯”ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
    # PPS å¯¹æ¯”ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
    # ...

    return result

def calculate_diff(ebpf_val: float, baseline_val: float) -> dict:
    diff_absolute = ebpf_val - baseline_val
    diff_percent = (diff_absolute / baseline_val) * 100 if baseline_val != 0 else 0

    return {
        "ebpf": round(ebpf_val, 2),
        "baseline": round(baseline_val, 2),
        "diff_absolute": round(diff_absolute, 2),
        "diff_percent": round(diff_percent, 2)
    }
```

**ç±»è®¾è®¡ï¼š**
```python
class BaselineComparator:
    @staticmethod
    def compare(ebpf_data: dict, baseline_data: dict) -> dict:
        pass

    @staticmethod
    def _calculate_diff(ebpf_val: float, baseline_val: float) -> dict:
        pass

    @staticmethod
    def _compare_latency(ebpf: dict, baseline: dict) -> dict:
        pass

    @staticmethod
    def _compare_throughput(ebpf: dict, baseline: dict) -> dict:
        pass

    @staticmethod
    def _compare_pps(ebpf: dict, baseline: dict) -> dict:
        pass
```

---

### 2.6 Report Generatorï¼ˆæŠ¥å‘Šç”Ÿæˆå™¨ï¼‰

**èŒè´£ï¼š** ç”Ÿæˆæ±‡æ€»è¡¨æ ¼ï¼ˆCSVã€Markdownï¼‰

**è¾“å…¥ï¼š**
- `topic`: Topic åç§°
- `results`: æ‰€æœ‰ tool cases çš„åˆ†æç»“æœåˆ—è¡¨

**è¾“å‡ºï¼š** ç”Ÿæˆæ–‡ä»¶åˆ° output ç›®å½•

#### 2.6.1 CSV Generator

**è¾“å‡ºæ ¼å¼ï¼š**
```csv
Tool Case,Protocol,Direction,Latency Mean (us),Latency Diff (%),Throughput Single Client (Gbps),Throughput Single Diff (%),PPS Single Client,PPS Single Diff (%),CPU Avg (%) - PPS Single,Memory Max (KB) - PPS Single,Max RSS (KB),Log Size (Bytes)
case_1,tcp,rx,110.5,5.2,12.05,-3.6,4500000,-2.1,15.3,146992,146992,0
```

**ç”Ÿæˆé€»è¾‘ï¼š**
```python
def generate_csv(topic: str, results: list, output_path: str):
    headers = [
        "Tool Case", "Protocol", "Direction",
        "Latency Mean (us)", "Latency Diff (%)",
        "Throughput Single Client (Gbps)", "Throughput Single Diff (%)",
        "PPS Single Client", "PPS Single Diff (%)",
        "CPU Avg (%) - PPS Single", "Memory Max (KB) - PPS Single",
        "Max RSS (KB)", "Log Size (Bytes)"
    ]

    rows = []
    for result in results:
        row = extract_row_data(result)
        rows.append(row)

    with open(output_path, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(headers)
        writer.writerows(rows)
```

#### 2.6.2 Markdown Generator

**è¾“å‡ºæ ¼å¼ï¼š**
```markdown
# Topic Name - Summary Report

**Iteration:** iteration_001
**Date:** 2025-10-22

| Tool Case | Protocol | Direction | Latency Mean (us) | ... |
|-----------|----------|-----------|-------------------|-----|
| case_1    | tcp      | rx        | 110.5             | ... |
```

**ç±»è®¾è®¡ï¼š**
```python
class ReportGenerator:
    def __init__(self, output_dir: str):
        self.output_dir = output_dir

    def generate(self, topic: str, results: list, iteration: str):
        """ç”Ÿæˆæ‰€æœ‰æ ¼å¼çš„æŠ¥å‘Š"""
        self.generate_csv(topic, results, iteration)
        self.generate_markdown(topic, results, iteration)

    def generate_csv(self, topic: str, results: list, iteration: str):
        pass

    def generate_markdown(self, topic: str, results: list, iteration: str):
        pass

    def _extract_row_data(self, result: dict) -> list:
        """ä»ç»“æœå­—å…¸æå–è¡¨æ ¼è¡Œæ•°æ®"""
        pass
```

---

## 3. å·¥å…·å‡½æ•°ï¼ˆutils.pyï¼‰

### 3.1 æ—¶é—´è½¬æ¢å‡½æ•°

```python
def parse_datetime(datetime_str: str) -> datetime:
    """è§£ææ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²

    æ”¯æŒæ ¼å¼ï¼š
    - 2025-10-21 14:12:43.774
    - Tue, 21 Oct 2025 14:13:41 GMT
    """
    pass

def datetime_to_epoch(datetime_str: str) -> int:
    """å°†æ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸º Unix æ—¶é—´æˆ³"""
    pass

def epoch_to_datetime(epoch: int) -> str:
    """å°† Unix æ—¶é—´æˆ³è½¬æ¢ä¸ºå¯è¯»å­—ç¬¦ä¸²"""
    pass
```

### 3.2 æ•°æ®å•ä½è½¬æ¢

```python
def humanize_bytes(bytes: int) -> str:
    """è½¬æ¢å­—èŠ‚æ•°ä¸ºäººç±»å¯è¯»æ ¼å¼

    Examples:
        0 -> "0B"
        1024 -> "1.0KB"
        1048576 -> "1.0MB"
    """
    pass

def bps_to_gbps(bps: float) -> float:
    """å°† bps è½¬æ¢ä¸º Gbps"""
    return round(bps / 1e9, 2)
```

### 3.3 æ–‡ä»¶æ“ä½œ

```python
def find_latest_file(pattern: str) -> str:
    """æ‰¾åˆ°åŒ¹é… glob æ¨¡å¼çš„æœ€æ–°æ–‡ä»¶"""
    import glob
    files = glob.glob(pattern)
    if not files:
        return None
    return sorted(files)[-1]

def safe_read_json(file_path: str) -> dict:
    """å®‰å…¨è¯»å– JSON æ–‡ä»¶ï¼Œå¤„ç†å¼‚å¸¸"""
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except Exception as e:
        logger.warning(f"Failed to read JSON {file_path}: {e}")
        return None
```

### 3.4 Tool Case åç§°è§£æ

```python
def parse_tool_case_name(tool_case_name: str) -> dict:
    """è§£æ tool case åç§°

    Input: "system_network_performance_case_6_tcp_tx_0388a9"
    Output: {
        "topic": "system_network_performance",
        "case_number": 6,
        "protocol": "tcp",
        "direction": "tx",
        "hash": "0388a9"
    }
    """
    import re
    pattern = r"(.+)_case_(\d+)_(\w+)_(\w+)_(\w+)"
    match = re.match(pattern, tool_case_name)

    if not match:
        return None

    return {
        "topic": match.group(1),
        "case_number": int(match.group(2)),
        "protocol": match.group(3),
        "direction": match.group(4),
        "hash": match.group(5)
    }
```

---

## 4. ä¸»ç¨‹åºè®¾è®¡

### 4.1 ä¸»æµç¨‹

```python
def main():
    # 1. åŠ è½½é…ç½®
    config = load_config("config.yaml")

    # 2. åˆå§‹åŒ–ç»„ä»¶
    iteration_path = os.path.join(config["data_root"], config["selected_iteration"])
    locator = DataLocator(iteration_path)
    report_gen = ReportGenerator(config["output_dir"])

    # 3. è·å–æ‰€æœ‰ topics
    topics = get_all_topics(iteration_path, config)

    # 4. å¤„ç†æ¯ä¸ª topic
    for topic in topics:
        logger.info(f"Processing topic: {topic}")

        # 4.1 è·å–è¯¥ topic çš„æ‰€æœ‰ tool cases
        tool_cases = get_tool_cases_for_topic(iteration_path, topic)

        # 4.2 è§£æ baseline
        test_type = detect_test_type_for_topic(topic)
        baseline_paths = locator.locate_baseline(test_type)
        baseline_data = PerformanceParser.parse_all(baseline_paths)

        # 4.3 å¤„ç†æ¯ä¸ª tool case
        results = []
        for tool_case in tool_cases:
            try:
                result = process_tool_case(
                    locator, tool_case, baseline_data, config
                )
                results.append(result)
            except Exception as e:
                logger.error(f"Failed to process {tool_case}: {e}")
                continue

        # 4.4 ç”ŸæˆæŠ¥å‘Š
        report_gen.generate(topic, results, config["selected_iteration"])

def process_tool_case(locator, tool_case_name, baseline_data, config):
    """å¤„ç†å•ä¸ª tool case"""
    # 1. å®šä½æ•°æ®
    paths = locator.locate_tool_case(tool_case_name)

    # 2. è§£ææ€§èƒ½æ•°æ®
    perf_data = PerformanceParser.parse_all(paths)

    # 3. è§£æèµ„æºç›‘æ§æ•°æ®
    time_ranges = extract_time_ranges(perf_data)
    resource_data = ResourceParser.parse(
        paths["server"]["ebpf_monitoring"]["resource_monitor"],
        time_ranges
    )

    # 4. è§£ææ—¥å¿—å¤§å°
    log_data = LogSizeParser.parse(
        paths["server"]["ebpf_monitoring"]["logsize_monitor"]
    )

    # 5. å¯¹æ¯” baseline
    comparison = BaselineComparator.compare(perf_data, baseline_data)

    # 6. è¿”å›æ±‡æ€»ç»“æœ
    return {
        "tool_case": tool_case_name,
        "metadata": parse_tool_case_name(tool_case_name),
        "performance": perf_data,
        "resources": resource_data,
        "logs": log_data,
        "comparison": comparison
    }
```

### 4.2 å‘½ä»¤è¡Œå‚æ•°

```python
import argparse

def parse_args():
    parser = argparse.ArgumentParser(
        description="Analyze performance test results"
    )

    parser.add_argument(
        "--iteration",
        type=str,
        default="iteration_001",
        help="Iteration to analyze (default: iteration_001)"
    )

    parser.add_argument(
        "--topic",
        type=str,
        default=None,
        help="Specific topic to analyze (default: all topics)"
    )

    parser.add_argument(
        "--config",
        type=str,
        default="config.yaml",
        help="Path to configuration file (default: config.yaml)"
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        default="./output",
        help="Output directory (default: ./output)"
    )

    parser.add_argument(
        "--format",
        type=str,
        default="csv,markdown",
        help="Output formats (default: csv,markdown)"
    )

    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose logging"
    )

    return parser.parse_args()
```

---

## 5. é…ç½®æ–‡ä»¶è®¾è®¡

```yaml
# config.yaml

# æ•°æ®æ ¹ç›®å½•
data_root: "../results"

# å¾…åˆ†æçš„ iterations
iterations:
  - iteration_001
  - iteration_002
  - iteration_003

# ä¼˜å…ˆåˆ†æçš„ iteration
selected_iteration: iteration_001

# Topics é…ç½®
topics:
  host:
    - system_network_performance
    - linux_network_stack
  vm:
    - kvm_virt_network
    - ovs_monitoring
    - vm_network_performance

# è¾“å‡ºé…ç½®
output_dir: "./output"
output_formats:
  - csv
  - markdown

# æ€§èƒ½å·®å¼‚é˜ˆå€¼ï¼ˆç”¨äºé«˜äº®ï¼‰
thresholds:
  latency_degradation_percent: 5.0
  throughput_degradation_percent: 5.0
  pps_degradation_percent: 5.0

# æ—¥å¿—é…ç½®
logging:
  level: INFO
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "./analysis.log"
```

---

## 6. é”™è¯¯å¤„ç†ç­–ç•¥

### 6.1 æ•°æ®ç¼ºå¤±å¤„ç†

**ç­–ç•¥ï¼š**
1. å…³é”®æ•°æ®ç¼ºå¤±ï¼šè®°å½•è­¦å‘Šï¼Œè¯¥ tool case æ ‡è®°ä¸º "N/A"ï¼Œç»§ç»­å¤„ç†å…¶ä»– cases
2. æ¬¡è¦æ•°æ®ç¼ºå¤±ï¼šä½¿ç”¨é»˜è®¤å€¼å¡«å……ï¼Œåœ¨æŠ¥å‘Šä¸­æ·»åŠ æ³¨é‡Š

**å®ç°ï¼š**
```python
def safe_parse(parser_func, *args, default=None):
    """å®‰å…¨è§£æï¼Œæ•è·å¼‚å¸¸å¹¶è¿”å›é»˜è®¤å€¼"""
    try:
        return parser_func(*args)
    except FileNotFoundError as e:
        logger.warning(f"File not found: {e}")
        return default
    except Exception as e:
        logger.error(f"Parse error: {e}")
        return default
```

### 6.2 æ•°æ®æ ¼å¼å¼‚å¸¸

**ç­–ç•¥ï¼š**
1. JSON è§£æå¤±è´¥ï¼šè®°å½•é”™è¯¯ï¼Œè¿”å› None
2. CSV æ ¼å¼ä¸åŒ¹é…ï¼šå°è¯•çµæ´»è§£æï¼Œå¤±è´¥åˆ™è·³è¿‡
3. æ•°å€¼è½¬æ¢å¤±è´¥ï¼šä½¿ç”¨ 0 æˆ– NaN

---

## 7. æµ‹è¯•ç­–ç•¥

### 7.1 å•å…ƒæµ‹è¯•

**æµ‹è¯•æ¨¡å—ï¼š**
- `test_data_locator.py`: æµ‹è¯•æ–‡ä»¶å®šä½é€»è¾‘
- `test_parsers.py`: æµ‹è¯•å„è§£æå™¨çš„æ­£ç¡®æ€§
- `test_comparator.py`: æµ‹è¯•å¯¹æ¯”è®¡ç®—
- `test_utils.py`: æµ‹è¯•å·¥å…·å‡½æ•°

**æµ‹è¯•æ•°æ®ï¼š**
- ä½¿ç”¨ `tests/fixtures/` å­˜æ”¾æµ‹è¯•æ•°æ®æ ·æœ¬

### 7.2 é›†æˆæµ‹è¯•

**æµ‹è¯•æµç¨‹ï¼š**
1. ä½¿ç”¨çœŸå®çš„ `iteration_001` æ•°æ®è¿è¡Œå®Œæ•´åˆ†æ
2. éªŒè¯è¾“å‡ºæ–‡ä»¶ç”Ÿæˆæ­£ç¡®
3. æ‰‹å·¥æ£€æŸ¥éƒ¨åˆ†ç»“æœçš„å‡†ç¡®æ€§

---

## 8. æ€§èƒ½ä¼˜åŒ–è€ƒè™‘

### 8.1 å¹¶è¡Œå¤„ç†

å¯¹äºå¤šä¸ª tool cases çš„å¤„ç†ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨å¤šè¿›ç¨‹ï¼š

```python
from multiprocessing import Pool

def parallel_process_tool_cases(tool_cases, locator, baseline_data, config):
    with Pool(processes=4) as pool:
        results = pool.starmap(
            process_tool_case,
            [(locator, tc, baseline_data, config) for tc in tool_cases]
        )
    return results
```

### 8.2 æ•°æ®ç¼“å­˜

å¯¹äºé‡å¤è¯»å–çš„ baseline æ•°æ®ï¼Œå¯ä»¥ä½¿ç”¨ç¼“å­˜ï¼š

```python
from functools import lru_cache

@lru_cache(maxsize=10)
def load_baseline_cached(baseline_path):
    return PerformanceParser.parse_all(baseline_path)
```

---

## 9. æ‰©å±•æ€§è®¾è®¡

### 9.1 æ–°å¢è¾“å‡ºæ ¼å¼

**æ¥å£è®¾è®¡ï¼š**
```python
class ReportGenerator:
    def generate(self, topic, results, iteration):
        for fmt in self.formats:
            generator = self._get_generator(fmt)
            generator.generate(topic, results, iteration)

    def _get_generator(self, fmt):
        if fmt == "csv":
            return CSVGenerator(self.output_dir)
        elif fmt == "markdown":
            return MarkdownGenerator(self.output_dir)
        elif fmt == "excel":
            return ExcelGenerator(self.output_dir)
        else:
            raise ValueError(f"Unknown format: {fmt}")
```

### 9.2 æ–°å¢è§£æå™¨

åªéœ€å®ç°æ–°çš„ Parser ç±»å¹¶åœ¨ä¸»æµç¨‹ä¸­è°ƒç”¨å³å¯ã€‚

---

## 10. å®‰å…¨æ€§è€ƒè™‘

### 10.1 è·¯å¾„æ³¨å…¥

**é˜²æŠ¤ï¼š**
```python
def safe_join_path(base, *parts):
    """å®‰å…¨æ‹¼æ¥è·¯å¾„ï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»"""
    path = os.path.join(base, *parts)
    if not os.path.abspath(path).startswith(os.path.abspath(base)):
        raise ValueError("Invalid path")
    return path
```

### 10.2 èµ„æºé™åˆ¶

**é˜²æŠ¤ï¼š**
- é™åˆ¶å•ä¸ªæ—¥å¿—æ–‡ä»¶æœ€å¤§è¯»å–å¤§å°ï¼ˆé¿å… OOMï¼‰
- è®¾ç½®è¶…æ—¶æœºåˆ¶

---

## 11. æ–‡æ¡£è¾“å‡º

### 11.1 ä½¿ç”¨è¯´æ˜ï¼ˆREADME.mdï¼‰

åŒ…å«ï¼š
- å¿«é€Ÿå¼€å§‹
- é…ç½®è¯´æ˜
- å‘½ä»¤è¡Œå‚æ•°
- è¾“å‡ºæ ¼å¼è¯´æ˜
- å¸¸è§é—®é¢˜

### 11.2 å¼€å‘æ–‡æ¡£

åŒ…å«ï¼š
- æ¶æ„è®¾è®¡ï¼ˆæœ¬æ–‡æ¡£ï¼‰
- API æ–‡æ¡£ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
- è´¡çŒ®æŒ‡å—

---

## 10. æŠ¥å‘Šç”Ÿæˆå™¨ V2 è®¾è®¡ï¼ˆå½“å‰å®ç°ï¼‰

### 10.1 è®¾è®¡ç›®æ ‡

è§£å†³åŸå§‹åˆå¹¶æŠ¥å‘Šçš„é—®é¢˜ï¼š
- âŒ åˆ—æ•°è¿‡å¤šï¼ˆ37åˆ—ï¼‰ï¼Œéš¾ä»¥é˜…è¯»
- âŒ Excel éœ€è¦æ¨ªå‘æ»šåŠ¨
- âŒ ä¸åŒç±»å‹æŒ‡æ ‡æ··åœ¨ä¸€èµ·

æ–°è®¾è®¡ç›®æ ‡ï¼š
- âœ… æŒ‰æŒ‡æ ‡ç±»å‹åˆ†ç¦»æŠ¥å‘Š
- âœ… æ¯ä¸ªæŠ¥å‘Šåˆ—æ•°åˆç†ï¼ˆ13-20åˆ—ï¼‰
- âœ… æä¾›æ¦‚è§ˆ Markdown å¿«é€ŸæŸ¥çœ‹

### 10.2 ç±»è®¾è®¡

```python
class ReportGeneratorV2:
    """Enhanced report generator with separated report types"""
    
    def __init__(self, output_dir: str):
        self.output_dir = output_dir
    
    def generate_all(self, topic: str, results: List[Dict], iteration: str):
        """Generate all types of reports"""
        self.generate_latency_report(topic, results, iteration)
        self.generate_throughput_report(topic, results, iteration)
        self.generate_pps_report(topic, results, iteration)
        self.generate_resources_report(topic, results, iteration)
        self.generate_overview_markdown(topic, results, iteration)
```

### 10.3 æŠ¥å‘Šç±»å‹è¯¦ç»†è®¾è®¡

#### 10.3.1 å»¶è¿ŸæŠ¥å‘Šï¼ˆLatency Reportï¼‰

**æ–‡ä»¶å**: `{topic}_latency_{iteration}.csv`

**æ•°æ®æå–é€»è¾‘**ï¼š
```python
def _extract_latency_row(self, result: Dict) -> List:
    """ä» result["performance"]["client"]["latency"] æå–æ•°æ®"""
    # TCP RR æ•°æ®
    tcp_rr = result["performance"]["client"]["latency"]["tcp_rr"]
    tcp_comp = result["comparison"]["latency"]["tcp_rr_mean_us"]
    
    # UDP RR æ•°æ®
    udp_rr = result["performance"]["client"]["latency"]["udp_rr"]
    udp_comp = result["comparison"]["latency"]["udp_rr_mean_us"]
    
    return [
        tool_case, protocol, direction,
        tcp_rr["min_us"], tcp_rr["mean_us"], tcp_rr["max_us"],
        tcp_comp["baseline"], tcp_comp["diff_percent"],
        udp_rr["min_us"], udp_rr["mean_us"], udp_rr["max_us"],
        udp_comp["baseline"], udp_comp["diff_percent"]
    ]
```

#### 10.3.2 ååé‡æŠ¥å‘Šï¼ˆThroughput Reportï¼‰

**æ–‡ä»¶å**: `{topic}_throughput_{iteration}.csv`

**æ•°æ®æå–é€»è¾‘**ï¼š
```python
def _extract_throughput_row(self, result: Dict) -> List:
    """ä» result["comparison"]["throughput_client/server"] æå–æ•°æ®"""
    # Client æ•°æ®
    client_single = result["comparison"]["throughput_client"]["single_gbps"]
    client_multi = result["comparison"]["throughput_client"]["multi_gbps"]
    
    # Server æ•°æ®
    server_single = result["comparison"]["throughput_server"]["single_gbps"]
    server_multi = result["comparison"]["throughput_server"]["multi_gbps"]
    
    return [
        tool_case, protocol, direction,
        client_single["ebpf"], client_single["baseline"], client_single["diff_percent"],
        client_multi["ebpf"], client_multi["baseline"], client_multi["diff_percent"],
        server_single["ebpf"], server_single["baseline"], server_single["diff_percent"],
        server_multi["ebpf"], server_multi["baseline"], server_multi["diff_percent"]
    ]
```

#### 10.3.3 PPS æŠ¥å‘Šï¼ˆPPS Reportï¼‰

**è®¾è®¡**: ä¸ Throughput æŠ¥å‘Šç»“æ„ç›¸åŒï¼Œä½†å€¼ä¸º PPS

#### 10.3.4 èµ„æºæŠ¥å‘Šï¼ˆResources Reportï¼‰

**æ–‡ä»¶å**: `{topic}_resources_{iteration}.csv`

**æ•°æ®æå–é€»è¾‘**ï¼š
```python
def _extract_resources_row(self, result: Dict) -> List:
    """ä» result["resources"] å’Œ result["logs"] æå–æ•°æ®"""
    time_range_stats = result["resources"]["time_range_stats"]
    full_cycle = result["resources"]["full_cycle"]
    log_size = result["logs"]["log_size"]
    
    return [
        tool_case, protocol, direction,
        # PPS workload
        time_range_stats["pps_single"]["cpu"]["avg_percent"],
        time_range_stats["pps_single"]["cpu"]["max_percent"],
        time_range_stats["pps_single"]["memory"]["max_rss_kb"],
        # ... æ›´å¤šå­—æ®µ
        # Full cycle
        full_cycle["max_rss_kb"],
        full_cycle["max_vsz_kb"],
        # Log size
        log_size["final_size_bytes"],
        log_size["final_size_human"]
    ]
```

#### 10.3.5 æ¦‚è§ˆæŠ¥å‘Šï¼ˆOverview Markdownï¼‰

**æ–‡ä»¶å**: `{topic}_overview_{iteration}.md`

**ç”Ÿæˆé€»è¾‘**ï¼š
```python
def generate_overview_markdown(self, topic, results, iteration):
    """ç”Ÿæˆæ¦‚è§ˆ Markdown"""
    # 1. ç»Ÿè®¡æ‘˜è¦
    stats = self._calculate_summary_stats(results)
    
    # 2. æ€§èƒ½æ‘˜è¦è¡¨æ ¼ï¼ˆç²¾ç®€ç‰ˆï¼‰
    # åªæ˜¾ç¤º Tool Case, Protocol, Direction, ä¸‰å¤§æŒ‡æ ‡å·®å¼‚%
    
    # 3. è¯¦ç»†æŠ¥å‘Šæ–‡ä»¶åˆ—è¡¨
    # æŒ‡å‘å…¶ä»– 4 ä¸ª CSV æ–‡ä»¶
```

### 10.4 æ•°æ®æµç¨‹

```
analyze_performance.py (ä¸»ç¨‹åº)
    â†“
process_tool_case() â†’ ç”Ÿæˆ result å­—å…¸
    â†“
results = [result1, result2, ...] (æ‰€æœ‰ tool cases)
    â†“
æ ¹æ® --report-style å‚æ•°é€‰æ‹©ï¼š
    â”œâ”€â”€â”€ combined â†’ ReportGenerator.generate()
    â”‚    â””â”€â”€â”€ ç”Ÿæˆ 37 åˆ—çš„åˆå¹¶ CSV + Markdown
    â”‚
    â”œâ”€â”€â”€ separated â†’ ReportGeneratorV2.generate_all()
    â”‚    â””â”€â”€â”€ ç”Ÿæˆ 5 ä¸ªä¸“é¡¹æ–‡ä»¶
    â”‚
    â””â”€â”€â”€ both (é»˜è®¤) â†’ ä¸¤è€…éƒ½ç”Ÿæˆ
```

### 10.5 å‘½ä»¤è¡Œé›†æˆ

åœ¨ `analyze_performance.py` ä¸­æ·»åŠ å‚æ•°ï¼š

```python
parser.add_argument(
    "--report-style",
    type=str,
    choices=["combined", "separated", "both"],
    default="both",
    help="Report generation style (default: both)"
)

# ä½¿ç”¨
if args.report_style in ["combined", "both"]:
    report_gen.generate(topic, results, iteration, formats=config["output_formats"])

if args.report_style in ["separated", "both"]:
    report_gen_v2.generate_all(topic, results, iteration)
```

### 10.6 ä¼˜åŠ¿å¯¹æ¯”

| ç‰¹æ€§ | åˆå¹¶æŠ¥å‘Š | åˆ†ç¦»æŠ¥å‘Š |
|------|---------|---------|
| æ–‡ä»¶æ•° | 2ä¸ª | 5ä¸ª |
| CSV åˆ—æ•° | 37 | 13-20 |
| Excel é€‚é… | âŒ éœ€è¦æ»šåŠ¨ | âœ… å®Œæ•´æ˜¾ç¤º |
| å¯è¯»æ€§ | â­â­ | â­â­â­â­â­ |
| èšç„¦æ€§ | âŒ æ··åˆ | âœ… æŒ‰ç±»å‹åˆ†ç¦» |
| åˆ†ææ•ˆç‡ | ä½ | é«˜ |
| å…¼å®¹æ€§ | âœ… å‘åå…¼å®¹ | - |

**æ¨è**: æ—¥å¸¸åˆ†æä½¿ç”¨åˆ†ç¦»æŠ¥å‘Šï¼Œéœ€è¦å®Œæ•´æ•°æ®æ—¶ä½¿ç”¨åˆå¹¶æŠ¥å‘Šã€‚

---

## 11. å·²å®ç°çš„å…³é”®ä¿®å¤

### 11.1 Multi-stream Timing æ ¼å¼ä¿®å¤

**é—®é¢˜**: Multi-stream throughput çš„ timing æ ¼å¼ä¸ single stream ä¸åŒ

**è§£å†³æ–¹æ¡ˆ**: `src/parsers/performance_parser.py:131`

```python
# ä¿®æ”¹å‰
timing = PerformanceParser._parse_timing_log(timing_path, "throughput")

# ä¿®æ”¹å
timing = PerformanceParser._parse_timing_log(timing_path, "pps")  # Multi-stream ä½¿ç”¨ PPS æ ¼å¼
```

### 11.2 PPS Packet Size å®¹é”™

**é—®é¢˜**: éƒ¨åˆ† PPS JSON ç¼ºå°‘ `test_start` å­—æ®µ

**è§£å†³æ–¹æ¡ˆ**: `src/parsers/performance_parser.py:163-174`

```python
packet_size = None
# å°è¯• 1: data["test_start"]["blksize"]
if "test_start" in data and "blksize" in data["test_start"]:
    packet_size = data["test_start"]["blksize"]
# å°è¯• 2: data["start"]["test_start"]["blksize"]
elif "start" in data and "test_start" in data["start"]:
    packet_size = data["start"]["test_start"]["blksize"]
# é»˜è®¤å€¼
else:
    logger.warning("Using default packet size 64 bytes")
    packet_size = 64
```

### 11.3 Multi-stream æ•°æ®ç´¯åŠ 

**å®ç°**: `src/parsers/performance_parser.py:113-127`

```python
def parse_throughput_multi(json_paths, timing_path):
    total_bps = 0
    for json_path in json_paths:
        data = json.load(open(json_path))
        total_bps += data["end"]["sum_sent"]["bits_per_second"]
    
    throughput_gbps = total_bps / 1e9
    # è¿”å›ç´¯åŠ åçš„æ€»ååé‡
```

---

## 12. æ–‡æ¡£ä¸ä»£ç å¯¹åº”å…³ç³»

### 12.1 æ ¸å¿ƒæ–‡ä»¶æ¸…å•

| æ–‡ä»¶ | åŠŸèƒ½ | è¡Œæ•° | çŠ¶æ€ |
|------|------|------|------|
| `analyze_performance.py` | ä¸»ç¨‹åº | ~400 | âœ… |
| `src/data_locator.py` | æ•°æ®å®šä½ | ~350 | âœ… |
| `src/parsers/performance_parser.py` | æ€§èƒ½è§£æ | ~400 | âœ… |
| `src/parsers/resource_parser.py` | èµ„æºç›‘æ§è§£æ | ~200 | âœ… |
| `src/parsers/logsize_parser.py` | æ—¥å¿—å¤§å°è§£æ | ~100 | âœ… |
| `src/comparator.py` | Baseline å¯¹æ¯” | ~150 | âœ… |
| `src/report_generator.py` | åˆå¹¶æŠ¥å‘Š | ~300 | âœ… |
| `src/report_generator_v2.py` | åˆ†ç¦»æŠ¥å‘Š | ~500 | âœ… |
| `src/utils.py` | å·¥å…·å‡½æ•° | ~200 | âœ… |

### 12.2 é…ç½®æ–‡ä»¶

| æ–‡ä»¶ | åŠŸèƒ½ | çŠ¶æ€ |
|------|------|------|
| `config.yaml` | ä¸»é…ç½® | âœ… |
| `.gitignore` | Git å¿½ç•¥ | âœ… |

### 12.3 æ–‡æ¡£æ–‡ä»¶

| æ–‡ä»¶ | å†…å®¹ | çŠ¶æ€ |
|------|------|------|
| `REQUIREMENTS.md` | è¯¦ç»†éœ€æ±‚ | âœ… å·²æ›´æ–° |
| `DESIGN.md` | æ¶æ„è®¾è®¡ | âœ… å·²æ›´æ–° |
| `README.md` | ä½¿ç”¨è¯´æ˜ | âœ… |
| `QUICKSTART.md` | å¿«é€Ÿå¼€å§‹ | âœ… å·²æ›´æ–° |
| `FIXES_SUMMARY.md` | ä¿®å¤è®°å½• | âœ… |
| `OPTIMIZATION_SUMMARY.md` | ä¼˜åŒ–æ€»ç»“ | âœ… |

---

## 13. æ€»ç»“

æœ¬å·¥å…·é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œå„æ¨¡å—èŒè´£æ¸…æ™°ï¼š
- **æ•°æ®å®šä½**: DataLocator è‡ªåŠ¨è¯†åˆ«æµ‹è¯•ç±»å‹å’Œæ–‡ä»¶è·¯å¾„
- **æ•°æ®è§£æ**: 3 ä¸ªä¸“é¡¹ Parser å¤„ç†ä¸åŒç±»å‹æ•°æ®
- **æ•°æ®å¯¹æ¯”**: Comparator è®¡ç®—ä¸ baseline çš„å·®å¼‚
- **æŠ¥å‘Šç”Ÿæˆ**: åŒæŠ¥å‘Šç³»ç»Ÿæ»¡è¶³ä¸åŒéœ€æ±‚

**å…³é”®åˆ›æ–°**ï¼š
1. Multi-stream æ•°æ®ç´¯åŠ 
2. PPS packet_size å®¹é”™
3. åˆ†ç¦»æŠ¥å‘Šç³»ç»Ÿ

**ä»£ç è´¨é‡**ï¼š
- è¯¦ç»†çš„æ—¥å¿—è¾“å‡º
- å®Œå–„çš„å®¹é”™å¤„ç†
- æ¸…æ™°çš„ä»£ç æ³¨é‡Š
- å®Œæ•´çš„æ–‡æ¡£è¦†ç›–

å·¥å…·å·²è¾¾åˆ°ç”Ÿäº§å¯ç”¨çŠ¶æ€ï¼Œå¯ç”¨äºå®é™…çš„æ€§èƒ½åˆ†æå·¥ä½œã€‚
