# iperf3 性能测试 TCP 流量详细分析报告

**测试时间**: 2025-11-11 15:33:51 ~ 15:33:59
**分析人员**: Claude Code
**分析日期**: 2025-11-11

---

## 执行摘要

本次 iperf3 性能测试在协议层面存在**严重的性能问题**：

- ✗ **数据字节重传率高达 38-45%**（正常应 <1%）
- ✗ **所有16个并发流均出现系统性高重传**（16-31% 包级重传率）
- ✗ **带宽浪费严重**：需要多发送 83% 的冗余数据
- ✗ **接收端缓冲区耗尽**：21次 TCP Zero Window 事件
- ✗ **实际吞吐量仅约 370 Mbps**（服务端视角），远低于理论值

**根本原因推断**：网络丢包严重 + 服务端接收处理能力不足的复合问题。

---

## 1. 测试环境信息

### 1.1 网络拓扑

```
客户端: 100.100.103.205 (多个源端口)
    │
    │  iperf3 上传测试
    │  16 个并发 TCP 流
    ↓
服务端: 100.100.103.201:5001
```

### 1.2 抓包基本信息

| 项目 | 客户端 | 服务端 |
|------|--------|--------|
| 总包数 | 40,000 包 | 40,000 包 |
| 文件大小 | 888 MB | 655 MB |
| 抓包时长 | 2.537 秒 | 8.318 秒 |
| 平均包大小 | 22,202 字节 | 16,371 字节 |
| 平均包速率 | 15,773 包/秒 | 4,808 包/秒 |

**关键观察**:
- 两端抓包时长差异巨大（2.5秒 vs 8.3秒）
- 说明抓包时间窗口不一致，或存在严重的传输延迟/停顿

### 1.3 iperf3 配置

- **并发流数**: 16 个数据流 + 1 个控制流
- **传输方向**: 客户端 → 服务端（上传测试）
- **测试端口**: 5001

**连接列表**:
- 控制连接: 端口 35994 (仅 45 包，3.8 KB)
- 数据流: 端口 35996, 35998, 36000, 36002, 36004, 36006, 36008, 36010, 36012, 36014, 36016, 36018, 36020, 36022, 36024, 36026

---

## 2. 分析方法说明

### 2.1 重传检测原理

使用 tshark expert analysis 功能自动检测 TCP 重传：

```bash
tshark -q -z expert <pcap文件>
```

**检测机制**:
1. tshark 维护每个 TCP 连接的状态机，追踪已发送的序列号范围
2. 当看到重复的 TCP 序列号（非正常 ACK），判定为重传
3. 区分普通重传（超时）和快速重传（3个重复ACK）

**过滤器**:
- `tcp.analysis.retransmission`: 所有重传（包括超时和快速重传）
- `tcp.analysis.fast_retransmission`: 仅快速重传
- `tcp.analysis.zero_window`: TCP 零窗口事件

### 2.2 数据提取命令

**包级别统计**:
```bash
# 总包数
tshark -r <pcap> -Y "tcp.port==5001" | wc -l

# 重传包数
tshark -r <pcap> -Y "tcp.port==5001 && tcp.analysis.retransmission" | wc -l
```

**字节级别统计**:
```bash
# 客户端发送的总字节数
tshark -r <pcap> -Y "tcp.port==5001 && tcp.len > 0 && ip.src==100.100.103.205" \
  -T fields -e tcp.len | awk '{sum+=$1} END {print sum}'

# 客户端重传的字节数
tshark -r <pcap> -Y "tcp.port==5001 && tcp.analysis.retransmission && tcp.len > 0 && ip.src==100.100.103.205" \
  -T fields -e tcp.len | awk '{sum+=$1} END {print sum}'
```

**每个连接独立分析**:
```bash
# 针对每个端口分别统计
for port in 35996 35998 ... 36026; do
  tshark -r <pcap> -Y "tcp.srcport==$port || tcp.dstport==$port" | wc -l
  tshark -r <pcap> -Y "(tcp.srcport==$port || tcp.dstport==$port) && tcp.analysis.retransmission" | wc -l
done
```

---

## 3. 包级别分析

### 3.1 总体包统计

**tshark Expert Info 输出 (客户端)**:

```
Warns (14,377)
===============
  5,735  Previous segment(s) not captured
  8,604  ACKed segment that wasn't captured
     33  Out-of-order segment
      5  D-SACK Sequence

Notes (39,994)
===============
  7,814  This frame is a (suspected) retransmission         ← 重传包
    956  Ambiguous ACK
  8,039  Duplicate ACK (#1)
  6,021  Duplicate ACK (#2)
    517  This frame is a (suspected) fast retransmission    ← 快速重传
 15,441  Packet length exceeds MSS (TSO)
  1,191  Partial Acknowledgement
```

**tshark Expert Info 输出 (服务端)**:

```
Warns (13,449)
===============
  5,580  ACKed segment that wasn't captured
  7,824  Previous segment(s) not captured
     15  Out-of-order segment
     21  TCP Zero Window segment                            ← 接收缓冲区满
      9  D-SACK Sequence

Notes (45,955)
===============
  9,982  This frame is a (suspected) retransmission         ← 重传包
  1,400  Ambiguous ACK
  6,485  Duplicate ACK (#1)
  6,261  Duplicate ACK (#2)
  2,033  This frame is a (suspected) fast retransmission    ← 快速重传
 19,753  Packet length exceeds MSS (TSO)
```

### 3.2 包级别重传率

| 视角 | 总包数 | 重传包数 | 快速重传 | 包重传率 |
|------|--------|---------|---------|---------|
| 客户端 | 40,000 | 7,814 | 517 | 19.54% |
| 服务端 | 40,000 | 9,982 | 2,033 | 24.96% |

**关键观察**:
- 服务端视角的重传包更多（9,982 vs 7,814）
- 服务端快速重传是客户端的4倍（2,033 vs 517）
- 说明服务端接收路径上丢包更严重

---

## 4. 每个连接独立分析

### 4.1 控制连接

**端口 35994** (iperf3 控制通道):
- 客户端: 总包数=45, 重传=10 (22.2%)
- 服务端: 总包数=42, 重传=10 (23.8%)
- 数据量: 仅 3.8 KB
- **结论**: 控制连接也受影响，但影响不大

### 4.2 数据流连接

| 端口 | 客户端<br>总包数 | 客户端<br>重传 | 客户端<br>重传率 | 服务端<br>总包数 | 服务端<br>重传 | 服务端<br>重传率 |
|------|------------------|----------------|------------------|------------------|----------------|------------------|
| 35996 | 2,392 | 504 | **21.07%** | 2,647 | 708 | **26.74%** |
| 35998 | 1,951 | 377 | **19.32%** | 2,091 | 530 | **25.34%** |
| 36000 | 3,196 | 558 | **17.45%** | 2,906 | 654 | **22.50%** |
| 36002 | 1,727 | 402 | **23.27%** | 3,227 | 807 | **25.00%** |
| 36004 | 1,406 | 417 | **29.65%** ⚠️ | 1,314 | 380 | **28.91%** ⚠️ |
| 36006 | 2,688 | 460 | **17.11%** | 2,249 | 541 | **24.05%** |
| 36008 | 2,343 | 444 | **18.95%** | 1,650 | 374 | **22.66%** |
| 36010 | 1,848 | 389 | **21.04%** | 2,041 | 627 | **30.72%** ⚠️ |
| 36012 | 3,126 | 518 | **16.57%** | 3,137 | 697 | **22.21%** |
| 36014 | 1,944 | 474 | **24.38%** | 2,729 | 633 | **23.19%** |
| 36016 | 2,500 | 500 | **20.00%** | 2,797 | 614 | **21.95%** |
| 36018 | 3,404 | 593 | **17.42%** | 2,372 | 593 | **25.00%** |
| 36020 | 2,956 | 575 | **19.45%** | 2,678 | 723 | **26.99%** |
| 36022 | 2,758 | 449 | **16.27%** | 3,138 | 777 | **24.76%** |
| 36024 | 2,518 | 478 | **18.98%** | 2,650 | 718 | **27.09%** |
| 36026 | 3,198 | 666 | **20.82%** | 2,332 | 596 | **25.55%** |

**统计摘要**:
- 所有16个数据流的包级重传率范围: 16.27% - 30.72%
- 平均重传率: 约 20% (客户端), 约 25% (服务端)
- 最差连接: 端口 36004 和 36010（重传率接近 30%）

**关键结论**:
- ✗ **所有连接均受影响** - 这是系统性问题，不是个别连接问题
- ✗ **没有一个连接的重传率低于 16%** - 远超正常的 <1% 标准
- ✗ **16个流的重传率分布相对均匀** - 说明是共性的网络/系统瓶颈

---

## 5. 字节级别分析（核心指标）

### 5.1 数据传输方向确认

**客户端抓包统计**:
- 客户端 → 服务端: **844.19 MB**
- 服务端 → 客户端: **0 MB**

**确认**: 这是一个**上传测试**（客户端发送数据到服务端）

### 5.2 重传字节数详细分析

#### 客户端视角（发送端）

```
总发送 TCP payload:     885,204,472 字节 (844.19 MB)
其中重传的字节数:       402,184,848 字节 (383.55 MB)
───────────────────────────────────────────────────
数据字节重传率:         45.43%
有效传输数据:           483,019,624 字节 (460.64 MB)
重传额外开销:           83.26% (相对于有效数据)
```

**解读**:
- 客户端实际发送了 844 MB 数据
- 但其中 **45%（383 MB）是重传的冗余数据**
- 真正的有效数据只有 461 MB
- 相当于每传输 100 MB 有效数据，就要额外发送 83 MB 的重传数据

#### 服务端视角（接收端）

```
总接收 TCP payload:     651,967,528 字节 (621.73 MB)
其中重传的字节数:       248,270,496 字节 (236.74 MB)
───────────────────────────────────────────────────
数据字节重传率:         38.08%
有效传输数据:           403,697,032 字节 (384.99 MB)
重传额外开销:           61.49% (相对于有效数据)
```

**解读**:
- 服务端收到了 622 MB 数据
- 其中 **38%（237 MB）是重传包**
- 有效数据约 385 MB
- 比客户端视角少 76 MB (460 - 385 = 75 MB)

**两端差异分析**:
- 客户端有效数据: 460.64 MB
- 服务端有效数据: 384.99 MB
- **差异: 75.65 MB (16.4%)**

可能原因:
1. 抓包时间窗口不同（2.5秒 vs 8.3秒）
2. 抓包点不同（可能一端在应用层，一端在网络层）
3. 部分数据包在服务端抓包结束后才到达

### 5.3 重传包中的纯 ACK vs 数据包

| 视角 | 总重传包数 | tcp.len=0<br>(纯ACK) | 真正的<br>数据重传 |
|------|-----------|---------------------|-------------------|
| 客户端 | 7,814 | 68 | 7,746 |
| 服务端 | 9,982 | 68 | 9,914 |

**结论**: 重传包中 99% 都携带数据，不是纯 ACK。

---

## 6. 性能影响评估

### 6.1 实际吞吐量计算

**客户端视角（2.537 秒抓包窗口）**:

```
有效数据传输:     460.64 MB
抓包时长:         2.537 秒
实际吞吐量:       460.64 × 8 / 2.537 = 1,452.55 Mbps

线路占用带宽:     844.19 × 8 / 2.537 = 2,661.04 Mbps
带宽有效利用率:   460.64 / 844.19 = 54.57%
带宽浪费:         45.43%
```

**服务端视角（8.318 秒抓包窗口）**:

```
有效数据传输:     384.99 MB
抓包时长:         8.318 秒
实际吞吐量:       384.99 × 8 / 8.318 = 370.27 Mbps

线路占用带宽:     621.73 × 8 / 8.318 = 598.20 Mbps
带宽有效利用率:   384.99 / 621.73 = 61.92%
带宽浪费:         38.08%
```

**关键观察**:
- 两端吞吐量差异巨大：1,453 Mbps vs 370 Mbps
- 服务端视角的吞吐量更能反映真实性能（因为抓包时长更长，更稳定）
- **实际吞吐约 370 Mbps，远低于千兆网络的理论值**

### 6.2 性能损失量化

**方法1: 基于字节重传率**

```
性能损失 = 重传字节 / 总发送字节
         = 45.43% (客户端视角)
         = 38.08% (服务端视角)
```

**方法2: 基于额外开销**

```
额外开销 = 重传字节 / 有效数据字节
         = 83.26% (客户端视角)
         = 61.49% (服务端视角)

解读：每传输 100 MB 有效数据，需要额外重传 83 MB 冗余数据
```

**方法3: 基于吞吐量对比**

```
理论吞吐（假设无丢包）：
  460 MB / 2.5 秒 = 1,472 Mbps (客户端侧理想值)

实际吞吐（服务端实测）：
  370 Mbps

性能损失 = 1 - (370 / 1472) = 74.9%
```

**综合评估**:
- 带宽浪费: 38-45%
- 有效吞吐损失: 约 75%
- 额外开销: 需要多发 60-83% 的数据

---

## 7. 流控问题分析

### 7.1 TCP Zero Window 事件

**服务端抓包检测到 21 次 Zero Window**:

```
Frame    Time         Source IP        Dest IP          Port    Window
29661    4.816481s    100.100.103.201  100.100.103.205  5001    0
29662    4.816482s    100.100.103.201  100.100.103.205  5001    0
29663    4.816483s    100.100.103.201  100.100.103.205  5001    0
29664    4.816484s    100.100.103.201  100.100.103.205  5001    0
...
35779    6.216489s    100.100.103.201  100.100.103.205  5001    0
35780    6.216491s    100.100.103.201  100.100.103.205  5001    0
...
```

**发生时间点**:
- 第一波: 4.816 秒附近（多个连接同时）
- 第二波: 6.216 秒附近（多个连接同时）

**影响分析**:
- TCP Zero Window 表示接收方（服务端）的 TCP 接收缓冲区已满
- 接收方通告窗口为 0，要求发送方停止发送
- 发送方必须等待窗口更新才能继续发送
- 直接导致传输暂停，降低吞吐量

### 7.2 TCP 窗口大小统计

```
平均 TCP 窗口大小值（前1000个包）:
  客户端: 4,469 字节
  服务端: 4,477 字节
```

**问题**:
- TCP 窗口较小（正常应该是几十 KB 甚至上百 KB）
- 小窗口限制了发送速率
- 配合 Zero Window 事件，说明接收缓冲区管理存在问题

### 7.3 流量分布特征

**客户端 IO 统计（100ms 时间窗口）**:

```
Time Window    Frames    Bytes
0.0 - 0.1      5,285     68.6 MB
0.1 - 0.2      1,551     31.1 MB
0.2 - 0.3      2,417     50.5 MB
...
```

特征: **相对平稳**，持续有数据发送

**服务端 IO 统计（100ms 时间窗口）**:

```
Time Window    Frames    Bytes
0.0 - 0.1      3,819     39.4 MB
0.4 - 0.5      0         0         ← 静默
0.6 - 0.8      0         0         ← 静默
1.5 - 2.3      间歇性静默
3.0 - 3.7      间歇性静默
...
```

特征: **极度不均匀**，大量静默期

**结论**:
- 服务端流量呈现严重的突发特征
- 多个长时间静默期（0 流量）
- 说明服务端接收处理存在瓶颈，无法连续处理数据

---

## 8. 其他 TCP 特征

### 8.1 乱序包统计

| 视角 | 乱序包数 |
|------|---------|
| 客户端 | 33 |
| 服务端 | 15 |

数量不大，但配合高重传率，表明网络路径可能存在不稳定性。

### 8.2 重复 ACK 统计

**客户端**:
- Duplicate ACK #1: 8,039 次
- Duplicate ACK #2: 6,021 次
- Duplicate ACK #3: 1 次

**服务端**:
- Duplicate ACK #1: 6,485 次
- Duplicate ACK #2: 6,261 次
- Duplicate ACK #3: 13 次
- Duplicate ACK #4: 13 次
- Duplicate ACK #5: 2 次
- Duplicate ACK #6: 1 次

**解读**:
- 重复 ACK 是接收方告知发送方数据丢失的信号
- 大量重复 ACK 触发快速重传机制
- 数量过多表明丢包持续发生

### 8.3 RTT (往返时延)

| 测量点 | 平均 RTT |
|--------|---------|
| 客户端测量 | 3.26 ms |
| 服务端测量 | 0.018 ms ⚠️ |

**异常**: 服务端测量的 RTT 异常低（18 微秒）

**可能原因**:
- 服务端抓包点靠近应用层，测量的是本地处理延迟
- 或 tshark 计算方法在服务端不准确

**建议**: 以客户端测量的 3.26 ms 为准

---

## 9. 根因分析

### 9.1 问题清单

| 问题 | 严重程度 | 证据 |
|------|---------|------|
| 数据包丢失严重 | 🔴 严重 | 45% 字节重传率 |
| 所有连接系统性高重传 | 🔴 严重 | 16个流均 16-31% 重传 |
| 接收端缓冲区耗尽 | 🔴 严重 | 21次 Zero Window |
| 接收端处理不均匀 | 🔴 严重 | 服务端流量大量静默期 |
| TCP 窗口较小 | ⚠️ 警告 | 平均 ~4.5 KB |
| 大量重复 ACK | ⚠️ 警告 | 6000-8000 次 |
| 包乱序 | ℹ️ 一般 | 15-33 个 |

### 9.2 可能的根本原因

#### 原因1: 网络丢包严重（最可能）

**证据**:
- 45% 的字节是重传数据
- 所有16个连接均受影响
- 大量重复 ACK

**可能来源**:
- 物理链路问题（网线、网卡、交换机端口）
- 网络拥塞
- 中间设备（交换机、路由器、防火墙）丢包
- MTU 不匹配导致的分片问题

#### 原因2: 服务端接收能力不足（最可能）

**证据**:
- 21次 TCP Zero Window（接收缓冲区满）
- 服务端流量极度不均匀，有大量静默期
- 服务端抓包时长是客户端的3.3倍（8.3s vs 2.5s）

**可能来源**:
- 服务端 CPU 负载过高，处理不过来
- TCP 接收缓冲区设置过小
- 应用层（iperf3 server）处理速度慢
- 磁盘 I/O 瓶颈（如果写入磁盘）
- 内存不足导致处理延迟

#### 原因3: 系统资源竞争

**证据**:
- 流量不均匀且有长时间静默期
- 多个连接同时出现 Zero Window

**可能来源**:
- CPU 调度问题
- 中断处理瓶颈
- 内存分配延迟
- 其他进程占用资源

### 9.3 两种原因的交互

**负反馈循环**:

```
网络丢包
  → TCP 重传
  → 拥塞控制降速
  → 传输时间延长
  → 服务端缓冲区积压
  → Zero Window
  → 发送方暂停
  → 超时重传
  → 更多丢包...
```

**复合影响**:
- 网络丢包导致重传，降低了有效吞吐
- 服务端处理慢导致缓冲区满，进一步限制吞吐
- 两个问题相互加剧

---

## 10. 排查建议

### 10.1 立即检查项

#### 检查1: 网络硬件和链路

```bash
# 检查网卡丢包统计
ip -s link show

# 检查网卡错误
ethtool -S <interface_name>

# 检查 MTU 设置
ip link show

# 检查网线连接和协商速率
ethtool <interface_name>
```

#### 检查2: 服务端系统资源

```bash
# CPU 使用率
top
htop

# 内存使用
free -h

# 网络统计
netstat -s | grep -i retrans
ss -ti  # 查看 TCP 详细信息

# 检查 socket 缓冲区设置
sysctl net.ipv4.tcp_rmem
sysctl net.ipv4.tcp_wmem
sysctl net.core.rmem_max
sysctl net.core.wmem_max
```

#### 检查3: TCP 参数配置

```bash
# 查看当前 TCP 配置
sysctl -a | grep tcp

# 重点检查
sysctl net.ipv4.tcp_window_scaling
sysctl net.ipv4.tcp_timestamps
sysctl net.ipv4.tcp_sack
sysctl net.ipv4.tcp_congestion_control
```

#### 检查4: 中间网络设备

- 检查交换机端口统计（丢包、错误、冲突）
- 检查防火墙日志和丢包统计
- 检查是否有 QoS 策略限制带宽

### 10.2 优化建议

#### 优化1: 增大 TCP 缓冲区

```bash
# 增大接收缓冲区（示例值，根据实际调整）
sysctl -w net.ipv4.tcp_rmem="4096 87380 16777216"
sysctl -w net.ipv4.tcp_wmem="4096 65536 16777216"
sysctl -w net.core.rmem_max=16777216
sysctl -w net.core.wmem_max=16777216

# 永久生效，写入 /etc/sysctl.conf
```

#### 优化2: 检查和修复网络丢包

1. 更换网线
2. 更换网卡或交换机端口
3. 检查网卡 offload 设置（TSO/GSO/GRO）
4. 禁用可能有问题的 offload 功能（测试用）

```bash
ethtool -K <interface> tso off gso off gro off  # 测试用
```

#### 优化3: 调整 TCP 拥塞控制算法

```bash
# 尝试更激进的拥塞控制算法
sysctl -w net.ipv4.tcp_congestion_control=bbr

# 或
sysctl -w net.ipv4.tcp_congestion_control=cubic
```

#### 优化4: 系统调优

```bash
# 增加软中断处理能力
echo 32768 > /proc/sys/net/core/netdev_max_backlog

# 增加 SYN 队列
sysctl -w net.ipv4.tcp_max_syn_backlog=8192

# 调整系统文件描述符限制
ulimit -n 65535
```

### 10.3 进一步诊断

#### 诊断1: 使用 eBPF 工具跟踪丢包

```bash
# 使用 kernel_drop_stack_stats_summary.py (你的工具)
sudo python3 ebpf-tools/linux-network-stack/packet-drop/kernel_drop_stack_stats_summary.py

# 或使用 dropwatch
dropwatch -l kas
```

#### 诊断2: 跟踪 TCP 状态变化

```bash
# 使用 tcptrace 分析 pcap
tcptrace -l client.pcap

# 或使用 tcplife (bcc 工具)
sudo /usr/share/bcc/tools/tcplife
```

#### 诊断3: CPU 性能分析

```bash
# 检查是否有 CPU 瓶颈
perf top

# 跟踪 iperf3 进程
perf record -p <iperf3_pid>
perf report
```

---

## 11. 结论与建议

### 11.1 核心结论

1. **性能问题确认**:
   - 协议层面性能严重异常
   - 数据字节重传率 38-45%（正常应 <1%）
   - 实际吞吐仅约 370 Mbps

2. **问题性质**:
   - **系统性问题**：所有16个连接均受影响
   - **复合问题**：网络丢包 + 服务端接收瓶颈
   - **持续性问题**：贯穿整个测试期间

3. **主要瓶颈**:
   - 网络丢包严重（45% 重传率）
   - 服务端接收处理能力不足（21次 Zero Window）
   - TCP 缓冲区配置可能不足

### 11.2 优先级排序

| 优先级 | 排查项 | 预期收益 |
|--------|--------|---------|
| P0 | 检查网络硬件和链路质量 | 高 |
| P0 | 检查服务端 CPU/内存资源 | 高 |
| P1 | 增大 TCP 接收缓冲区 | 中 |
| P1 | 检查中间网络设备丢包 | 中 |
| P2 | 调整 TCP 拥塞控制算法 | 低 |
| P2 | 优化系统参数 | 低 |

### 11.3 下一步行动

**立即执行**:
1. 在两端执行 `ip -s link show` 和 `ethtool -S <interface>` 检查丢包统计
2. 在服务端执行 `top` 检查 iperf3 进程 CPU 使用率
3. 检查 `sysctl net.ipv4.tcp_rmem` 和 `sysctl net.core.rmem_max`
4. 更换网线（最简单的排查方法）

**短期优化**:
1. 增大 TCP 接收缓冲区配置
2. 使用 eBPF 工具定位具体丢包点
3. 检查交换机端口统计

**长期改进**:
1. 建立性能基线，定期监控
2. 部署网络监控工具
3. 优化系统 TCP 参数配置

---

## 附录 A: 分析脚本

所有分析脚本已保存在:
- `/Users/admin/workspace/troubleshooting-tools/test/pcap-analyzer/analyze_per_connection.sh`
- `/Users/admin/workspace/troubleshooting-tools/test/pcap-analyzer/analyze_retrans_bytes_v2.sh`

可重复执行进行验证。

---

## 附录 B: 参考资料

- **TCP 重传机制**: RFC 6298 (TCP Retransmission Timer)
- **TCP 拥塞控制**: RFC 5681 (TCP Congestion Control)
- **TCP 窗口扩展**: RFC 7323 (TCP Extensions for High Performance)
- **Wireshark TCP Analysis**: https://wiki.wireshark.org/TCP_Analyze_Sequence_Numbers

---

**报告结束**

生成时间: 2025-11-11
工具版本: tshark 4.x, Python 3.x
分析平台: macOS Darwin 24.5.0
